import datetime
import os
import json
from tts import speak

try:
    from stt import listen
except ImportError:
    listen = None

from ollama_api import ollama_chat

# --- MÃ©moire cognitive persistante ---
MEMORY_FILE = "data/william_memory.json"

def load_memory():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"facts": {}, "habits": {}, "feedback": [], "repetitions": {}}

def save_memory(mem):
    os.makedirs(os.path.dirname(MEMORY_FILE), exist_ok=True)
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(mem, f, indent=2, ensure_ascii=False)

memory = load_memory()

def add_fact(key, value):
    memory["facts"][key] = value
    save_memory(memory)

def record_habit(intent):
    memory["habits"][intent] = memory["habits"].get(intent, 0) + 1
    save_memory(memory)

def add_feedback(feedback):
    memory["feedback"].append({"ts": datetime.datetime.now().isoformat(), "feedback": feedback})
    save_memory(memory)

def record_repetition(text):
    key = text.strip().lower()
    memory["repetitions"][key] = memory["repetitions"].get(key, 0) + 1
    save_memory(memory)
    return memory["repetitions"][key]

# --- DÃ©tection d'intention/utilitÃ© ---
def is_code_question(user_input):
    mots_code = [
        "code", "python", "fonction", "programme", "script",
        "algorithm", "boucle", "variable", "java", "c++", "c#", "js", "javascript",
        "ligne de code", "erreur", "bug", "debug", "compilation", "instruction", "class", "mÃ©thode", "def ", "```"
    ]
    return any(m in user_input.lower() for m in mots_code) or "```" in user_input

def detect_intent(text):
    txt = text.lower()
    if any(w in txt for w in ["bonjour", "salut", "hello"]):
        return "salutation"
    if "heure" in txt:
        return "question_heure"
    if "date" in txt or "jour" in txt:
        return "question_date"
    if "merci" in txt:
        return "remerciement"
    if "diagnostic" in txt:
        return "diagnostic"
    if "aide" in txt:
        return "aide"
    if any(w in txt for w in ["au revoir", "bye", "exit", "quit"]):
        return "au_revoir"
    if any(w in txt for w in ["que vois-tu Ã  lâ€™Ã©cran", "que vois-tu Ã  l'ecran", "lis ce document", "scan Ã©cran", "scan ecran", "lis l'Ã©cran", "lis l'ecran"]):
        return "ocr"
    return "autre"

# --- Web search & OCR integration ---
def web_search(query):
    try:
        from websearch import bing_search
        result = bing_search(query)
        return result if result else "Aucun rÃ©sultat web pertinent trouvÃ©."
    except Exception as e:
        return f"Recherche web impossible : {e}"

def run_ocr():
    try:
        from ocr import read_screen
        return read_screen()
    except Exception as e:
        return f"Erreur OCR : {e}"

# --- Log/statistique proactive ---
LOG_FILE = "william_diagnostics/logs/errors.log"
def analyze_error_log(max_lines=200):
    if not os.path.exists(LOG_FILE):
        return {"count": 0, "recent_errors": [], "suggest_clear": False}
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()[-max_lines:]
    error_count = sum(1 for l in lines if "ERROR:" in l or "Erreur" in l)
    recent_errors = [l.strip() for l in lines if "ERROR:" in l or "Erreur" in l][-5:]
    suggest_clear = os.path.getsize(LOG_FILE) > 1_000_000
    return {"count": error_count, "recent_errors": recent_errors, "suggest_clear": suggest_clear}

def prompt_diagnostic_if_needed():
    stats = analyze_error_log()
    if stats["count"] > 10:
        speak("ğŸš¨ J'ai rencontrÃ© de nombreuses erreurs rÃ©centes. Voulez-vous lancer un diagnostic ou nettoyer les logs ('clear logs') ?")
    if stats["suggest_clear"]:
        speak("âš ï¸ Les logs dâ€™erreurs sont volumineux. Dites 'clear logs' pour les nettoyer.")

# --- GÃ©nÃ©ration de rÃ©ponse principale ---
def get_response(user_input, history):
    # Ajoute un prompt systÃ¨me si premiÃ¨re question Ã  Llama 3
    llama_history = history[:]
    if not any(m.get("role") == "system" for m in llama_history):
        llama_history = [
            {"role": "system", "content": "Tu es un assistant intelligent, expert en informatique, qui rÃ©ponds toujours en franÃ§ais de faÃ§on claire, concise et pÃ©dagogique. Si tu dois expliquer du code, fais-le pas Ã  pas, simplement."}
        ] + llama_history

    # MÃ©moire cognitive dans le contexte
    context_info = ""
    if memory["facts"]:
        facts_str = "\n".join(f"- {k}: {v}" for k, v in list(memory["facts"].items())[-5:])
        context_info += f"Connaissances prÃ©cÃ©demment retenuesâ€¯:\n{facts_str}\n"
    if memory["habits"]:
        hab, count = max(memory["habits"].items(), key=lambda x: x[1])
        if count >= 3:
            context_info += f"L'utilisateur pose souvent des questions de type '{hab}' ({count} fois rÃ©cemment). Adapte-toi.\n"

    llama_history = llama_history[-8:]
    history = history[-8:]

    intent = detect_intent(user_input)
    record_habit(intent)
    repetition_count = record_repetition(user_input)

    # OCR prioritÃ© si demandÃ© explicitement
    if intent == "ocr":
        ocr_text = run_ocr()
        if ocr_text:
            add_fact(f"OCR du {datetime.datetime.now().isoformat()}", ocr_text)
            return f"Texte OCR extrait :\n{ocr_text[:500]}" + ("..." if len(ocr_text) > 500 else "")

    # GÃ©nÃ©ration code ou gÃ©nÃ©ral
    if is_code_question(user_input):
        code_response = ollama_chat(
            user_input + " (RÃ©ponds seulement avec le code ou la correction, sans explication superflue, maximum 10 lignes.)",
            history, model="deepseek-coder:latest"
        )
        prompt_llama = (
            f"{context_info}Voici la rÃ©ponse d'une IA spÃ©cialisÃ©e en code :\n{code_response}\n"
            "Explique ou reformule cette rÃ©ponse en franÃ§ais, de faÃ§on claire, pÃ©dagogique et concise. RÃ©ponds en 2 phrases maximum."
        )
        final_response = ollama_chat(prompt_llama, llama_history, model="llama3:latest")
    else:
        prompt = f"{context_info}{user_input} (RÃ©ponds en 2 phrases maximum, en franÃ§ais, direct au but.)"
        final_response = ollama_chat(prompt, llama_history, model="llama3:latest")

    # Recherche web auto si LLM ne sait pas
    if any(x in str(final_response).lower() for x in ["je ne sais pas", "je n'ai pas la rÃ©ponse"]):
        speak("Je vais chercher sur le web, un instant...")
        web_answer = web_search(user_input)
        final_response = final_response + "\nğŸ” D'aprÃ¨s le web :\n" + str(web_answer)

    # Feedback/satisfaction utilisateur
    if "merci" in user_input.lower():
        add_feedback("merci")
        nb_merci = sum(1 for f in memory["feedback"] if "merci" in f["feedback"])
        if nb_merci in [3, 5, 10]:
            speak("Merci de votre confiance ! Si vous souhaitez m'aider Ã  m'amÃ©liorer, dites-le moi ou donnez-moi un feedback.")
    if repetition_count >= 3:
        speak(f"Vous avez posÃ© plusieurs fois la mÃªme question : '{user_input.strip()}'. Voulez-vous de l'aide ou souhaitez-vous lancer un diagnostic ?")

    return final_response[:300]

# ---- MAIN PROGRAMME ----

print("ğŸ¤– Initialisation de William...")
print(f"ğŸ“… {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

try:
    from diagnostic import run_diagnostic
    erreurs = run_diagnostic()
    modules_critiques = [m for m, info in erreurs.items() if info["status"] == "ERROR" and m in ["wcm", "tts"]]
    if modules_critiques:
        print("\nâŒ Impossible de dÃ©marrer William : modules critiques manquants.")
        exit(1)
except Exception as e:
    print(f"âš ï¸ Erreur pendant le diagnostic : {e}")
    print("â›” DÃ©marrage annulÃ© pour Ã©viter des comportements instables.")
    exit(1)

print("\nğŸ¤– William est prÃªt ! Tapez 'quit' pour quitter.")

history = []
log_check_counter = 0

while True:
    mode = input("Mode [t]exte ou [v]ocal ? (t/v): ").strip().lower()
    if mode == "v" and listen:
        user_input = listen()
    else:
        user_input = input("ğŸ‘¤ Vous: ").strip()
    if not user_input:
        continue

    if user_input.lower() in ["quit", "exit"]:
        print("ğŸ‘‹ Au revoir !")
        break

    if user_input.lower() == "diagnostic":
        try:
            run_diagnostic()
        except Exception:
            print("Diagnostic indisponible.")
        continue

    if user_input.lower() == "clear logs":
        try:
            if os.path.exists(LOG_FILE):
                os.remove(LOG_FILE)
                print("ğŸ§¹ Les logs ont Ã©tÃ© nettoyÃ©s.")
            else:
                print("Aucun log Ã  nettoyer.")
        except Exception as e:
            print(f"Impossible de nettoyer les logsâ€¯: {e}")
        continue

    response = get_response(user_input, history)
    print(f"ğŸ¤– William: {response}")
    speak(response)

    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": response})

    # VÃ©rification proactive des logs/statistiques toutes les 10 interactions
    log_check_counter += 1
    if log_check_counter % 10 == 0:
        prompt_diagnostic_if_needed()